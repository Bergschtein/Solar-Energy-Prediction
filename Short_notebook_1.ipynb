{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short notebook 1\n",
    "\n",
    "### CatBoost model\n",
    "\n",
    "Name: Erlend Lokna, Student ID: 528564\n",
    "\n",
    "Name: Johan Vik Mathisen, Student ID: 508258\n",
    "\n",
    "\n",
    "Team name: Shaky Warriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that really should be in a separate file.\n",
    "\n",
    "Data is assumed to be available as in the handout. \n",
    "\n",
    "For example `pd.read_parquet('data/A/train_targets.parquet')` is assumed to work at the current file location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        kind: observerd, estimated, train\n",
    "        \"\"\"\n",
    "\n",
    "        train_a = pd.read_parquet('data/A/train_targets.parquet')\n",
    "        train_b = pd.read_parquet('data/B/train_targets.parquet')\n",
    "        train_c = pd.read_parquet('data/C/train_targets.parquet')\n",
    "\n",
    "        # Estimated training data for each location\n",
    "        X_train_estimated_a = pd.read_parquet('data/A/X_train_estimated.parquet')\n",
    "        X_train_estimated_b = pd.read_parquet('data/B/X_train_estimated.parquet')\n",
    "        X_train_estimated_c = pd.read_parquet('data/C/X_train_estimated.parquet')\n",
    "\n",
    "        # Observed training data for each location\n",
    "        X_train_observed_b = pd.read_parquet('data/B/X_train_observed.parquet')\n",
    "        X_train_observed_a = pd.read_parquet('data/A/X_train_observed.parquet')\n",
    "        X_train_observed_c = pd.read_parquet('data/C/X_train_observed.parquet')\n",
    "\n",
    "        # Estimated test data for each location\n",
    "        X_test_estimated_b = pd.read_parquet('data/B/X_test_estimated.parquet')\n",
    "        X_test_estimated_a = pd.read_parquet('data/A/X_test_estimated.parquet')\n",
    "        X_test_estimated_c = pd.read_parquet('data/C/X_test_estimated.parquet')\n",
    "\n",
    "        Y_train = {\n",
    "            'a': train_a, \n",
    "            'b':train_b, \n",
    "            'c':train_c\n",
    "        }\n",
    "        X_train_estimated = {\n",
    "            'a':X_train_estimated_a,\n",
    "            'b':X_train_estimated_b,\n",
    "            'c':X_train_estimated_c\n",
    "        }\n",
    "        X_train_observed = {\n",
    "            'a':X_train_observed_a,\n",
    "            'b':X_train_observed_b,\n",
    "            'c':X_train_observed_c\n",
    "        }\n",
    "        X_test_estimated = {\n",
    "            'a':X_test_estimated_a,\n",
    "            'b':X_test_estimated_b,\n",
    "            'c':X_test_estimated_c\n",
    "        }\n",
    "        self.X_train_observed =  X_train_observed\n",
    "        self.X_train_estimated = X_train_estimated\n",
    "        self.X_test_estimated = X_test_estimated\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "    def resample_to_hourly(self):\n",
    "        for loc in ['a','b','c']:\n",
    "            self.X_train_observed[loc] = to_hourly(self.X_train_observed[loc])\n",
    "            self.X_train_estimated[loc] = to_hourly(self.X_train_estimated[loc])\n",
    "            self.X_test_estimated[loc] = to_hourly(self.X_test_estimated[loc])\n",
    "\n",
    "\n",
    "    def select_features(self, features):\n",
    "        \"\"\" \n",
    "        Reduces dim by selecting only features from \"features\"\n",
    "        This will remove \"date_calc\" from est.\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            self.X_train_observed[loc] = self.X_train_observed[loc][features]\n",
    "            self.X_train_estimated[loc] = self.X_train_estimated[loc][features]\n",
    "            self.X_test_estimated[loc] = self.X_test_estimated[loc][features]\n",
    "\n",
    "    def add_type(self):\n",
    "        \"\"\"\n",
    "        0: Estimated data\n",
    "        1: Observed data\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            type_vec_X_tr = [1] * len(self.X_train_observed[loc])\n",
    "            self.X_train_observed[loc]['type'] = type_vec_X_tr\n",
    "\n",
    "            type_vec_X_tr_e = [0] * len(self.X_train_estimated[loc])\n",
    "            self.X_train_estimated[loc]['type'] = type_vec_X_tr_e\n",
    "\n",
    "            type_vec_X_te = [0] * len(self.X_test_estimated[loc])\n",
    "            self.X_test_estimated[loc]['type'] = type_vec_X_te\n",
    "\n",
    "\n",
    "    def add_location(self):\n",
    "        \"\"\"\n",
    "        Adds a categorical feature \"location\" equal to the input string location.\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            loc_vec_X_tr = [loc] * len(self.X_train_observed[loc])\n",
    "            self.X_train_observed[loc]['location'] = loc_vec_X_tr\n",
    "\n",
    "            loc_vec_X_tr_e = [loc] * len(self.X_train_estimated[loc])\n",
    "            self.X_train_estimated[loc]['location'] = loc_vec_X_tr_e\n",
    "\n",
    "            loc_vec_X_te = [loc] * len(self.X_test_estimated[loc])\n",
    "            self.X_test_estimated[loc]['location'] = loc_vec_X_te\n",
    "\n",
    "    def remove_nans(self, feature):\n",
    "        for loc in ['a','b','c']:\n",
    "            cols = self.X_train_observed['a'].columns\n",
    "            if feature in cols:\n",
    "                self.X_train_observed[loc] = self.X_train_observed[loc].dropna(subset = [feature], how = 'all')\n",
    "                self.X_train_estimated[loc] = self.X_train_estimated[loc].dropna(subset = [feature], how = 'all')\n",
    "                self.X_test_estimated[loc] = self.X_test_estimated[loc].dropna(subset = [feature], how = 'all')\n",
    "            else:\n",
    "                print(\"Feature not in data frame.\")\n",
    "\n",
    "    def combine_obs_est(self):\n",
    "        \"\"\"\n",
    "        Concatinates the estimated and observed data. \n",
    "        Removes data_calc from est.\n",
    "        \"\"\"\n",
    "\n",
    "        obs_a = self.X_train_observed['a']\n",
    "        est_a = self.X_train_estimated['a']\n",
    "\n",
    "        obs_b = self.X_train_observed['b']\n",
    "        est_b = self.X_train_estimated['b']\n",
    "\n",
    "        obs_c = self.X_train_observed['c']\n",
    "        est_c = self.X_train_estimated['c']\n",
    "\n",
    "        self.X_train = {\n",
    "        'a':pd.concat([obs_a, est_a]),\n",
    "        'b':pd.concat([obs_b, est_b]),\n",
    "        'c':pd.concat([obs_c, est_c])\n",
    "        }\n",
    "\n",
    "        self.X_train['a'] = self.X_train['a'].reset_index(drop=True)\n",
    "        self.X_train['b'] = self.X_train['b'].reset_index(drop=True)\n",
    "        self.X_train['c'] = self.X_train['c'].reset_index(drop=True)\n",
    "\n",
    "        self.X_train['a'], self.Y_train['a'] = match_X_Y(self.X_train['a'], self.Y_train['a'])\n",
    "        self.X_train['b'], self.Y_train['b'] = match_X_Y(self.X_train['b'], self.Y_train['b'])\n",
    "        self.X_train['c'], self.Y_train['c'] = match_X_Y(self.X_train['c'], self.Y_train['c'])\n",
    "    \n",
    "    def train_test(self):\n",
    "        \"\"\"\n",
    "        Vanilla split. \n",
    "        \"\"\"\n",
    "        X_a = self.X_train['a']\n",
    "        X_b = self.X_train['b']\n",
    "        X_c = self.X_train['c']\n",
    "\n",
    "        y_a = self.Y_train['a']\n",
    "        y_b = self.Y_train['b']\n",
    "        y_c = self.Y_train['c']\n",
    "\n",
    "        y_train = pd.concat([y_a, y_b, y_c])\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "        X_train = pd.concat([X_a, X_b, X_c])\n",
    "        X_test = pd.concat([self.X_test_estimated['a'], self.X_test_estimated['b'],self.X_test_estimated['c']])\n",
    "        \n",
    "        return X_train, X_test, y_train\n",
    "\n",
    "    def scale_y_train(self, k_b = 5, k_c = 6):\n",
    "\n",
    "        self.Y_train['b'] = self.Y_train['b'] * k_b \n",
    "        self.Y_train['c'] = self.Y_train['c']* k_c\n",
    "\n",
    "    def drop_bad_data(self):\n",
    "        for loc in ['a', 'b', 'c']:\n",
    "            y_ind = get_constant_indices(self.Y_train[loc])\n",
    "            self.Y_train[loc].drop(y_ind, errors='ignore')\n",
    "            self.X_train[loc].drop(y_ind, errors='ignore')\n",
    "\n",
    "\n",
    "    def cyclic_time_encoding(self):\n",
    "        for loc in ['a', 'b', 'c']:\n",
    "            for time_feature in [\"time\", \"date_forecast\"]:\n",
    "                if time_feature in self.X_train[loc].columns:\n",
    "                    self.X_train[loc]['sin_hour'] = np.sin(2*np.pi*self.X_train[loc][time_feature].dt.hour/24)\n",
    "                    self.X_train[loc]['sin_month'] = np.sin(2*np.pi*self.X_train[loc][time_feature].dt.month/12)\n",
    "\n",
    "                    self.X_train[loc]['cos_hour'] = np.cos(2*np.pi*self.X_train[loc][time_feature].dt.hour/24)\n",
    "                    self.X_train[loc]['cos_month'] = np.cos(2*np.pi*self.X_train[loc][time_feature].dt.month/12)\n",
    "                if time_feature in self.X_test_estimated[loc].columns:    \n",
    "                    self.X_test_estimated[loc]['sin_hour'] = np.sin(2*np.pi*self.X_test_estimated[loc][time_feature].dt.hour/24)\n",
    "                    self.X_test_estimated[loc]['sin_month'] = np.sin(2*np.pi*self.X_test_estimated[loc][time_feature].dt.month/12)\n",
    "\n",
    "                    self.X_test_estimated[loc]['cos_hour'] = np.cos(2*np.pi*self.X_test_estimated[loc][time_feature].dt.hour/24)\n",
    "                    self.X_test_estimated[loc]['cos_month'] = np.cos(2*np.pi*self.X_test_estimated[loc][time_feature].dt.month/12)\n",
    "\n",
    "#Helper functions\n",
    "\n",
    "def match_X_Y(X,Y):\n",
    "    \"\"\" \n",
    "    date_forecast and time must be unique!\n",
    "    Matches the timestamps of X to the timestamps of Y. \n",
    "    Makes sure that the length of X and Y are equal.\n",
    "    \"\"\"\n",
    "    Y = Y.dropna()\n",
    "    X = X.rename(columns={'date_forecast': 'time'})\n",
    "    merge_df = Y.merge(X, on=\"time\", how='inner')\n",
    "    Y = merge_df['pv_measurement']\n",
    "    X = merge_df.drop(columns = ['pv_measurement'])\n",
    "    return X,Y\n",
    "\n",
    "def to_hourly(df):\n",
    "    df['date_forecast']\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('H').mean()\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def make_categorical(data, feature_list):\n",
    "    for feature in feature_list:\n",
    "        data[feature] = data[feature].astype('category')\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def remap(x):\n",
    "    if x<0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_constant_indices(ser):\n",
    "    mask = (ser != 0)\n",
    "    constant_periods = ser[mask].groupby((ser[mask] != ser[mask].shift()).cumsum()).cumcount().add(1)\n",
    "    \n",
    "    drop_mask = constant_periods >= 12\n",
    "    return constant_periods[drop_mask].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['date_forecast', 'absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "       'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_6h:cm', 'is_day:idx',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_depth:cm',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms']\n",
    "\n",
    "made_features = ['location', 'type', 'is_day:idx', 'is_in_shadow:idx', 'dew_or_rime:idx']\n",
    "\n",
    "drop_feature = 'diffuse_rad:W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all data\n",
    "data_collection = DataSet()\n",
    "#Preprocessing\n",
    "data_collection.select_features(selected_features)\n",
    "data_collection.resample_to_hourly()\n",
    "data_collection.remove_nans(drop_feature)\n",
    "data_collection.add_location()\n",
    "data_collection.add_type()\n",
    "data_collection.combine_obs_est()\n",
    "data_collection.drop_bad_data()\n",
    "data_collection.cyclic_time_encoding()\n",
    "\n",
    "k_b = 5\n",
    "k_c = 6\n",
    "data_collection.scale_y_train(k_b = k_b, k_c = k_c)\n",
    "\n",
    "X_train, X_test, y_train = data_collection.train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in made_features:\n",
    "    if f not in ['location', 'type']:\n",
    "        X_train[f] = X_train[f].map(remap)\n",
    "        X_test[f] = X_test[f].map(remap)\n",
    "\n",
    "make_categorical(X_train,made_features)\n",
    "X_train = X_train.drop('time', axis=1)\n",
    "\n",
    "make_categorical(X_test,made_features)\n",
    "X_test = X_test.drop('date_forecast', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = cb.Pool(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cat_features = made_features\n",
    ")\n",
    "test_pool = cb.Pool(\n",
    "    X_test,\n",
    "    cat_features = made_features\n",
    ")\n",
    "\n",
    "model = cb.CatBoostRegressor(\n",
    "    iterations = 10000,\n",
    "    depth = 9,\n",
    "    learning_rate =0.005,\n",
    "    loss_function ='MAE',\n",
    "    cat_features = made_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(train_pool, silent=True)\n",
    "# make the prediction using the resulting model\n",
    "preds_original = model.predict(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale back\n",
    "length = int((X_test.shape[0]/3))\n",
    "pred_a = preds_original[:length]\n",
    "pred_b = preds_original[length:2*length] / k_b\n",
    "pred_c = preds_original[2*length:3*length] / k_c\n",
    "preds = np.concatenate([pred_a,pred_b, pred_c])\n",
    "preds = pd.DataFrame({'predictions':preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop low values\n",
    "preds['predictions'] = preds['predictions'].apply(lambda x: 0 if x < 5 else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = preds.reset_index()\n",
    "preds = preds.rename(columns={'index': 'id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds.to_csv('Short_notebook_1', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
