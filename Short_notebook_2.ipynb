{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short 2\n",
    "\n",
    "### Ensamble model of AutoGluon and Catboost\n",
    "\n",
    "Name: Erlend Lokna, Student ID: 528564\n",
    "\n",
    "Name: Johan Vik Mathisen, Student ID: 508258\n",
    "\n",
    "\n",
    "Team name: Shaky Warriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johanvikmathisen/Desktop/Fag/Matematikk/Solar-Energy-Prediction/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import catboost as cb\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        kind: observerd, estimated, train\n",
    "        \"\"\"\n",
    "\n",
    "        train_a = pd.read_parquet('data/A/train_targets.parquet')\n",
    "        train_b = pd.read_parquet('data/B/train_targets.parquet')\n",
    "        train_c = pd.read_parquet('data/C/train_targets.parquet')\n",
    "\n",
    "        # Estimated training data for each location\n",
    "        X_train_estimated_a = pd.read_parquet('data/A/X_train_estimated.parquet')\n",
    "        X_train_estimated_b = pd.read_parquet('data/B/X_train_estimated.parquet')\n",
    "        X_train_estimated_c = pd.read_parquet('data/C/X_train_estimated.parquet')\n",
    "\n",
    "        # Observed training data for each location\n",
    "        X_train_observed_b = pd.read_parquet('data/B/X_train_observed.parquet')\n",
    "        X_train_observed_a = pd.read_parquet('data/A/X_train_observed.parquet')\n",
    "        X_train_observed_c = pd.read_parquet('data/C/X_train_observed.parquet')\n",
    "\n",
    "        # Estimated test data for each location\n",
    "        X_test_estimated_b = pd.read_parquet('data/B/X_test_estimated.parquet')\n",
    "        X_test_estimated_a = pd.read_parquet('data/A/X_test_estimated.parquet')\n",
    "        X_test_estimated_c = pd.read_parquet('data/C/X_test_estimated.parquet')\n",
    "\n",
    "        Y_train = {\n",
    "            'a': train_a, \n",
    "            'b':train_b, \n",
    "            'c':train_c\n",
    "        }\n",
    "        X_train_estimated = {\n",
    "            'a':X_train_estimated_a,\n",
    "            'b':X_train_estimated_b,\n",
    "            'c':X_train_estimated_c\n",
    "        }\n",
    "        X_train_observed = {\n",
    "            'a':X_train_observed_a,\n",
    "            'b':X_train_observed_b,\n",
    "            'c':X_train_observed_c\n",
    "        }\n",
    "        X_test_estimated = {\n",
    "            'a':X_test_estimated_a,\n",
    "            'b':X_test_estimated_b,\n",
    "            'c':X_test_estimated_c\n",
    "        }\n",
    "        self.X_train_observed =  X_train_observed\n",
    "        self.X_train_estimated = X_train_estimated\n",
    "        self.X_test_estimated = X_test_estimated\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "    def resample_to_hourly(self):\n",
    "        for loc in ['a','b','c']:\n",
    "            self.X_train_observed[loc] = to_hourly(self.X_train_observed[loc])\n",
    "            self.X_train_estimated[loc] = to_hourly(self.X_train_estimated[loc])\n",
    "            self.X_test_estimated[loc] = to_hourly(self.X_test_estimated[loc])\n",
    "\n",
    "\n",
    "    def select_features(self, features):\n",
    "        \"\"\" \n",
    "        Reduces dim by selecting only features from \"features\"\n",
    "        This will remove \"date_calc\" from est.\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            self.X_train_observed[loc] = self.X_train_observed[loc][features]\n",
    "            self.X_train_estimated[loc] = self.X_train_estimated[loc][features]\n",
    "            self.X_test_estimated[loc] = self.X_test_estimated[loc][features]\n",
    "\n",
    "    def add_type(self):\n",
    "        \"\"\"\n",
    "        0: Estimated data\n",
    "        1: Observed data\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            type_vec_X_tr = [1] * len(self.X_train_observed[loc])\n",
    "            self.X_train_observed[loc]['type'] = type_vec_X_tr\n",
    "\n",
    "            type_vec_X_tr_e = [0] * len(self.X_train_estimated[loc])\n",
    "            self.X_train_estimated[loc]['type'] = type_vec_X_tr_e\n",
    "\n",
    "            type_vec_X_te = [0] * len(self.X_test_estimated[loc])\n",
    "            self.X_test_estimated[loc]['type'] = type_vec_X_te\n",
    "\n",
    "\n",
    "    def add_location(self):\n",
    "        \"\"\"\n",
    "        Adds a categorical feature \"location\" equal to the input string location.\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            loc_vec_X_tr = [loc] * len(self.X_train_observed[loc])\n",
    "            self.X_train_observed[loc]['location'] = loc_vec_X_tr\n",
    "\n",
    "            loc_vec_X_tr_e = [loc] * len(self.X_train_estimated[loc])\n",
    "            self.X_train_estimated[loc]['location'] = loc_vec_X_tr_e\n",
    "\n",
    "            loc_vec_X_te = [loc] * len(self.X_test_estimated[loc])\n",
    "            self.X_test_estimated[loc]['location'] = loc_vec_X_te\n",
    "\n",
    "    def remove_nans(self, feature):\n",
    "        for loc in ['a','b','c']:\n",
    "            cols = self.X_train_observed['a'].columns\n",
    "            if feature in cols:\n",
    "                self.X_train_observed[loc] = self.X_train_observed[loc].dropna(subset = [feature], how = 'all')\n",
    "                self.X_train_estimated[loc] = self.X_train_estimated[loc].dropna(subset = [feature], how = 'all')\n",
    "                self.X_test_estimated[loc] = self.X_test_estimated[loc].dropna(subset = [feature], how = 'all')\n",
    "            else:\n",
    "                print(\"Feature not in data frame.\")\n",
    "\n",
    "    def combine_obs_est(self):\n",
    "        \"\"\"\n",
    "        Concatinates the estimated and observed data. \n",
    "        Removes data_calc from est.\n",
    "        \"\"\"\n",
    "\n",
    "        obs_a = self.X_train_observed['a']\n",
    "        est_a = self.X_train_estimated['a']\n",
    "\n",
    "        obs_b = self.X_train_observed['b']\n",
    "        est_b = self.X_train_estimated['b']\n",
    "\n",
    "        obs_c = self.X_train_observed['c']\n",
    "        est_c = self.X_train_estimated['c']\n",
    "\n",
    "        self.X_train = {\n",
    "        'a':pd.concat([obs_a, est_a]),\n",
    "        'b':pd.concat([obs_b, est_b]),\n",
    "        'c':pd.concat([obs_c, est_c])\n",
    "        }\n",
    "\n",
    "        self.X_train['a'] = self.X_train['a'].reset_index(drop=True)\n",
    "        self.X_train['b'] = self.X_train['b'].reset_index(drop=True)\n",
    "        self.X_train['c'] = self.X_train['c'].reset_index(drop=True)\n",
    "\n",
    "        self.X_train['a'], self.Y_train['a'] = match_X_Y(self.X_train['a'], self.Y_train['a'])\n",
    "        self.X_train['b'], self.Y_train['b'] = match_X_Y(self.X_train['b'], self.Y_train['b'])\n",
    "        self.X_train['c'], self.Y_train['c'] = match_X_Y(self.X_train['c'], self.Y_train['c'])\n",
    "    \n",
    "    def train_test(self):\n",
    "        \"\"\"\n",
    "        Vanilla split. \n",
    "        \"\"\"\n",
    "        X_a = self.X_train['a']\n",
    "        X_b = self.X_train['b']\n",
    "        X_c = self.X_train['c']\n",
    "\n",
    "        y_a = self.Y_train['a']\n",
    "        y_b = self.Y_train['b']\n",
    "        y_c = self.Y_train['c']\n",
    "\n",
    "        y_train = pd.concat([y_a, y_b, y_c])\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "        X_train = pd.concat([X_a, X_b, X_c])\n",
    "        X_test = pd.concat([self.X_test_estimated['a'], self.X_test_estimated['b'],self.X_test_estimated['c']])\n",
    "        \n",
    "        return X_train, X_test, y_train\n",
    "\n",
    "    def scale_y_train(self, k_b = 5, k_c = 6):\n",
    "\n",
    "        self.Y_train['b'] = self.Y_train['b'] * k_b \n",
    "        self.Y_train['c'] = self.Y_train['c']* k_c\n",
    "\n",
    "    def drop_bad_data(self):\n",
    "        for loc in ['a', 'b', 'c']:\n",
    "            y_ind = get_constant_indices(self.Y_train[loc])\n",
    "            self.Y_train[loc].drop(y_ind, errors='ignore')\n",
    "            self.X_train[loc].drop(y_ind, errors='ignore')\n",
    "\n",
    "\n",
    "    def cyclic_time_encoding(self):\n",
    "        for loc in ['a', 'b', 'c']:\n",
    "            for time_feature in [\"time\", \"date_forecast\"]:\n",
    "                if time_feature in self.X_train[loc].columns:\n",
    "                    self.X_train[loc]['sin_hour'] = np.sin(2*np.pi*self.X_train[loc][time_feature].dt.hour/24)\n",
    "                    self.X_train[loc]['sin_month'] = np.sin(2*np.pi*self.X_train[loc][time_feature].dt.month/12)\n",
    "\n",
    "                    self.X_train[loc]['cos_hour'] = np.cos(2*np.pi*self.X_train[loc][time_feature].dt.hour/24)\n",
    "                    self.X_train[loc]['cos_month'] = np.cos(2*np.pi*self.X_train[loc][time_feature].dt.month/12)\n",
    "                if time_feature in self.X_test_estimated[loc].columns:    \n",
    "                    self.X_test_estimated[loc]['sin_hour'] = np.sin(2*np.pi*self.X_test_estimated[loc][time_feature].dt.hour/24)\n",
    "                    self.X_test_estimated[loc]['sin_month'] = np.sin(2*np.pi*self.X_test_estimated[loc][time_feature].dt.month/12)\n",
    "\n",
    "                    self.X_test_estimated[loc]['cos_hour'] = np.cos(2*np.pi*self.X_test_estimated[loc][time_feature].dt.hour/24)\n",
    "                    self.X_test_estimated[loc]['cos_month'] = np.cos(2*np.pi*self.X_test_estimated[loc][time_feature].dt.month/12)\n",
    "\n",
    "#Helper functions\n",
    "\n",
    "def match_X_Y(X,Y):\n",
    "    \"\"\" \n",
    "    date_forecast and time must be unique!\n",
    "    Matches the timestamps of X to the timestamps of Y. \n",
    "    Makes sure that the length of X and Y are equal.\n",
    "    \"\"\"\n",
    "    Y = Y.dropna()\n",
    "    X = X.rename(columns={'date_forecast': 'time'})\n",
    "    merge_df = Y.merge(X, on=\"time\", how='inner')\n",
    "    Y = merge_df['pv_measurement']\n",
    "    X = merge_df.drop(columns = ['pv_measurement'])\n",
    "    return X,Y\n",
    "\n",
    "def to_hourly(df):\n",
    "    df['date_forecast']\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('H').mean()\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def make_categorical(data, feature_list):\n",
    "    for feature in feature_list:\n",
    "        data[feature] = data[feature].astype('category')\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def remap(x):\n",
    "    if x<0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_constant_indices(ser):\n",
    "    mask = (ser != 0)\n",
    "    constant_periods = ser[mask].groupby((ser[mask] != ser[mask].shift()).cumsum()).cumcount().add(1)\n",
    "    \n",
    "    drop_mask = constant_periods >= 12\n",
    "    return constant_periods[drop_mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['date_forecast', 'absolute_humidity_2m:gm3',\n",
    "       'clear_sky_energy_1h:J', 'clear_sky_rad:W',\n",
    "       'cloud_base_agl:m', 'dew_or_rime:idx', 'dew_point_2m:K',\n",
    "       'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J',\n",
    "       'effective_cloud_cover:p', 'elevation:m', 'fresh_snow_12h:cm',\n",
    "       'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm',\n",
    "       'fresh_snow_6h:cm', 'is_in_shadow:idx', 'is_day:idx', \n",
    "       'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx',\n",
    "       'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p',\n",
    "       'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa',\n",
    "       'snow_depth:cm', 'snow_drift:idx',\n",
    "       'snow_melt_10min:mm', 'snow_water:kgm2', 'sun_azimuth:d',\n",
    "       'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K',\n",
    "       'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms',\n",
    "       'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms']\n",
    "\n",
    "made_features = ['location', 'type', 'is_day:idx', 'is_in_shadow:idx', 'dew_or_rime:idx']\n",
    "\n",
    "drop_feature = 'diffuse_rad:W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection = DataSet()\n",
    "data_collection.select_features(selected_features)\n",
    "data_collection.resample_to_hourly()\n",
    "data_collection.remove_nans(drop_feature)\n",
    "data_collection.add_location()\n",
    "data_collection.add_type()\n",
    "data_collection.combine_obs_est()\n",
    "data_collection.drop_bad_data()\n",
    "data_collection.cyclic_time_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = data_collection.X_train['a']\n",
    "X_b = data_collection.X_train['b']\n",
    "X_c = data_collection.X_train['c']\n",
    "\n",
    "y_a = data_collection.Y_train['a']\n",
    "y_b = data_collection.Y_train['b']\n",
    "y_c = data_collection.Y_train['c']\n",
    "\n",
    "for f in made_features:\n",
    "    if f not in ['location', 'type']:\n",
    "        X_a[f] = X_a[f].map(remap)\n",
    "        X_b[f] = X_b[f].map(remap)\n",
    "        X_c[f] = X_c[f].map(remap)\n",
    "\n",
    "make_categorical(X_a,made_features)\n",
    "make_categorical(X_b,made_features)\n",
    "make_categorical(X_c,made_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_cols = ['location', 'time']\n",
    "\n",
    "df_a = pd.concat([X_a, y_a], axis=1).drop(columns=drop_cols)\n",
    "df_b = pd.concat([X_b, y_b], axis=1).drop(columns=drop_cols)\n",
    "df_c = pd.concat([X_c, y_c], axis=1).drop(columns=drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 246\n",
    "\n",
    "data = dict()\n",
    "\n",
    "# sample 50% of the data for each building with type = 0\n",
    "df_a_tune = df_a[df_a['type'] == 0].sample(frac=0.5, random_state=seed)\n",
    "df_b_tune = df_b[df_b['type'] == 0].sample(frac=0.5, random_state=seed)   \n",
    "df_c_tune = df_c[df_c['type'] == 0].sample(frac=0.5, random_state=seed)\n",
    "\n",
    "# drop these rows from the original data\n",
    "df_a_train = df_a.drop(df_a_tune.index)\n",
    "df_b_train = df_b.drop(df_b_tune.index)\n",
    "df_c_train = df_c.drop(df_c_tune.index)\n",
    "\n",
    "data['a'] = [df_a_train, df_a_tune]\n",
    "data['b'] = [df_b_train, df_b_tune]\n",
    "data['c'] = [df_c_train, df_c_tune]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 hours (per model)\n",
    "time_in_sek = 60*60*2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'pv_measurement'\n",
    "predictor_a = TabularPredictor(label=label, eval_metric='mae').fit(\n",
    "    train_data = data['a'][0], \n",
    "    time_limit = time_in_sek,\n",
    "    presets='best_quality',\n",
    "    num_bag_folds=8,\n",
    "    num_stack_levels=0,\n",
    "    tuning_data = data['a'][1],\n",
    "    use_bag_holdout= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_b = TabularPredictor(label=label, eval_metric='mae').fit(\n",
    "    train_data = data['b'][0], \n",
    "    time_limit = time_in_sek,\n",
    "    presets='best_quality',\n",
    "    num_bag_folds=8,\n",
    "    num_stack_levels=0,\n",
    "    tuning_data = data['b'][1],\n",
    "    use_bag_holdout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_c = TabularPredictor(label=label, eval_metric='mae').fit(\n",
    "    train_data = data['c'][0], \n",
    "    time_limit = time_in_sek,\n",
    "    presets='best_quality',\n",
    "    num_bag_folds=8,\n",
    "    num_stack_levels=0,\n",
    "    tuning_data = data['c'][1],\n",
    "    use_bag_holdout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_a.refit_full()\n",
    "predictor_b.refit_full()\n",
    "predictor_c.refit_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_a.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_b.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_c.leaderboard(silent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = data_collection.X_test_estimated['a'].drop(columns=['location', 'date_forecast'])\n",
    "test_b = data_collection.X_test_estimated['b'].drop(columns=['location', 'date_forecast'])\n",
    "test_c = data_collection.X_test_estimated['c'].drop(columns=['location', 'date_forecast'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_a = predictor_a.predict(test_a)\n",
    "y_pred_b = predictor_b.predict(test_b)\n",
    "y_pred_c = predictor_c.predict(test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pd.concat([y_pred_a, y_pred_b, y_pred_c]).reset_index(drop=True)\n",
    "final_pred_AutoGluon = ReLU(final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['date_forecast', 'absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "       'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_6h:cm', 'is_day:idx',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_depth:cm',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms']\n",
    "\n",
    "made_features = ['location', 'type', 'is_day:idx', 'is_in_shadow:idx', 'dew_or_rime:idx']\n",
    "\n",
    "drop_feature = 'diffuse_rad:W'\n",
    "\n",
    "\n",
    "#Loading all data\n",
    "data_collection = DataSet()\n",
    "#Preprocessing\n",
    "data_collection.select_features(selected_features)\n",
    "data_collection.resample_to_hourly()\n",
    "data_collection.remove_nans(drop_feature)\n",
    "data_collection.add_location()\n",
    "data_collection.add_type()\n",
    "data_collection.combine_obs_est()\n",
    "data_collection.drop_bad_data()\n",
    "data_collection.cyclic_time_encoding()\n",
    "\n",
    "k_b = 5\n",
    "k_c = 6\n",
    "data_collection.scale_y_train(k_b = k_b, k_c = k_c)\n",
    "\n",
    "X_train, X_test, y_train = data_collection.train_test()\n",
    "\n",
    "for f in made_features:\n",
    "    if f not in ['location', 'type']:\n",
    "        X_train[f] = X_train[f].map(remap)\n",
    "        X_test[f] = X_test[f].map(remap)\n",
    "\n",
    "make_categorical(X_train,made_features)\n",
    "X_train = X_train.drop('time', axis=1)\n",
    "\n",
    "make_categorical(X_test,made_features)\n",
    "X_test = X_test.drop('date_forecast', axis=1)\n",
    "\n",
    "train_pool = cb.Pool(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cat_features = made_features\n",
    ")\n",
    "test_pool = cb.Pool(\n",
    "    X_test,\n",
    "    cat_features = made_features\n",
    ")\n",
    "\n",
    "model = cb.CatBoostRegressor(\n",
    "    iterations = 10000,\n",
    "    depth = 9,\n",
    "    learning_rate =0.005,\n",
    "    loss_function ='MAE',\n",
    "    cat_features = made_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(train_pool, silent=True)\n",
    "# make the prediction using the resulting model\n",
    "preds = model.predict(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale back\n",
    "length = int((X_test.shape[0]/3))\n",
    "pred_a = preds[:length]\n",
    "pred_b = preds[length:2*length] / k_b\n",
    "pred_c = preds[2*length:3*length] / k_c\n",
    "preds = np.concatenate([pred_a,pred_b, pred_c])\n",
    "#Drop negative values\n",
    "final_pred_cb = ReLU(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining for final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_AutoGluon = pd.DataFrame({'predictions':final_pred_AutoGluon})\n",
    "final_pred_AutoGluon['predictions'] = final_pred_AutoGluon['predictions'].apply(lambda x: 0 if x < 5 else x)\n",
    "\n",
    "final_pred_cb = pd.DataFrame({'predictions':final_pred_cb})\n",
    "final_pred_cb['predictions'] = final_pred_cb['predictions'].apply(lambda x: 0 if x < 5 else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = 0.5*(final_pred_AutoGluon + final_pred_cb)\n",
    "\n",
    "final_pred = final_pred.reset_index()\n",
    "final_pred = final_pred.rename(columns={'index': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred.to_csv('Short_notebook_2.csv', index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
